import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import os
import re
import yaml
import shutil
from docx import Document
from docx.shared import Inches
from docx.enum.table import WD_ALIGN_VERTICAL


class JokeParser:
    def __init__(self):
        self.base_url = "https://www.astrobl.ru"
        self.list_url = f"{self.base_url}/press/news"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36'
        }
        self.months_ru = {
            '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4,
            '–º–∞—è': 5, '–∏—é–Ω—è': 6, '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8,
            '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12
        }
        self.start_date, self.end_date, self.search_list, self.reports_path, self.config = self.load_date_config()

    def load_date_config(self):
        try:
            config_paths = [
                "parsers/config.yaml",
                "config.yaml",
                os.path.join(os.path.dirname(__file__), "config.yaml")
            ]
            config_path = None
            for path in config_paths:
                if os.path.exists(path):
                    config_path = path
                    break
            if config_path:
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                reports = config.get('reports', {})
                return (
                    reports.get('start_date'),
                    reports.get('end_date'),
                    reports.get('search_list', []),
                    reports.get('path', '../reports/'),
                    config,
                )
            return "2025-09-01", "2025-09-23", [], "../reports/", {}
        except Exception:
            return "2025-09-01", "2025-09-23", [], "../reports/", {}

    def format_period_ru(self, start_iso: str, end_iso: str) -> str:
        try:
            start_dt = datetime.strptime(start_iso, "%Y-%m-%d")
            end_dt = datetime.strptime(end_iso, "%Y-%m-%d")
        except Exception:
            return f"{start_iso} ‚Äî {end_iso}"
        months_gen = [
            "—è–Ω–≤–∞—Ä—è", "—Ñ–µ–≤—Ä–∞–ª—è", "–º–∞—Ä—Ç–∞", "–∞–ø—Ä–µ–ª—è", "–º–∞—è", "–∏—é–Ω—è",
            "–∏—é–ª—è", "–∞–≤–≥—É—Å—Ç–∞", "—Å–µ–Ω—Ç—è–±—Ä—è", "–æ–∫—Ç—è–±—Ä—è", "–Ω–æ—è–±—Ä—è", "–¥–µ–∫–∞–±—Ä—è"
        ]
        if start_dt.year == end_dt.year:
            if start_dt.month == end_dt.month:
                month_name = months_gen[end_dt.month - 1]
                return f"{start_dt.day} –ø–æ {end_dt.day} {month_name} {end_dt.year}"
            else:
                return f"{start_dt.day} {months_gen[start_dt.month-1]} –ø–æ {end_dt.day} {months_gen[end_dt.month-1]} {end_dt.year}"
        else:
            return f"{start_dt.day} {months_gen[start_dt.month-1]} {start_dt.year} –ø–æ {end_dt.day} {months_gen[end_dt.month-1]} {end_dt.year}"

    def check_keywords_in_title(self, title: str):
        if not self.search_list or not title:
            return False, []
        found = []
        for kw in self.search_list:
            # –°—Ç—Ä–æ–≥–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞ –∏ –≥—Ä–∞–Ω–∏—Ü —Å–ª–æ–≤–∞/—Ñ—Ä–∞–∑—ã
            pattern = r"(?<!\\w)" + re.escape(kw) + r"(?!\\w)"
            if re.search(pattern, title):  # –±–µ–∑ IGNORECASE
                found.append(kw)
        return len(found) > 0, found

    def fetch_article_text(self, url: str) -> str:
        html = self.get(url)
        if not html:
            return ''
        try:
            soup = BeautifulSoup(html, 'html.parser')
            article = soup.select_one('.news-text, [itemprop="articleBody"]')
            if not article:
                article = soup.select_one('.content__body_main')
            return article.get_text(' ', strip=True) if article else ''
        except Exception:
            return ''

    def check_keywords_in_text(self, text: str):
        if not self.search_list or not text:
            return False, []
        found = []
        for kw in self.search_list:
            pattern = r"(?<!\\w)" + re.escape(kw) + r"(?!\\w)"
            if re.search(pattern, text):  # –±–µ–∑ IGNORECASE
                found.append(kw)
        return len(found) > 0, found

    def get(self, url: str):
        try:
            resp = requests.get(url, headers=self.headers, timeout=15)
            resp.raise_for_status()
            return resp.text
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return None

    def build_period_param(self) -> str:
        # DD.MM.YYYY+-+DD.MM.YYYY
        try:
            sd = datetime.strptime(self.start_date, "%Y-%m-%d").strftime("%d.%m.%Y")
            ed = datetime.strptime(self.end_date, "%Y-%m-%d").strftime("%d.%m.%Y")
        except Exception:
            ed = datetime.now().strftime("%d.%m.%Y")
            sd = (datetime.now() - timedelta(days=3)).strftime("%d.%m.%Y")
        return f"{sd}+-+{ed}"

    def parse_date_text(self, text: str) -> str:
        # –ü—Ä–∏–º–µ—Ä—ã: "–°–µ–≥–æ–¥–Ω—è –≤ 13:13", "–í—á–µ—Ä–∞ –≤ 9:28", "–í—Ç–æ—Ä–Ω–∏–∫, 9:21", "23 —Å–µ–Ω—Ç—è–±—Ä—è 2025, 14:55"
        t = text.strip()
        today = datetime.now()
        if t.lower().startswith("—Å–µ–≥–æ–¥–Ω—è"):
            return today.strftime("%Y-%m-%d")
        if t.lower().startswith("–≤—á–µ—Ä–∞"):
            return (today - timedelta(days=1)).strftime("%Y-%m-%d")
        # –î–µ–Ω—å –Ω–µ–¥–µ–ª–∏, –Ω–∞–ø—Ä–∏–º–µ—Ä: "–í—Ç–æ—Ä–Ω–∏–∫, 9:21" ‚Üí –±–ª–∏–∂–∞–π—à–∏–π –ø—Ä–æ—à–µ–¥—à–∏–π –≤—Ç–æ—Ä–Ω–∏–∫
        weekdays = {
            '–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫': 0,
            '–≤—Ç–æ—Ä–Ω–∏–∫': 1,
            '—Å—Ä–µ–¥–∞': 2,
            '—á–µ—Ç–≤–µ—Ä–≥': 3,
            '–ø—è—Ç–Ω–∏—Ü–∞': 4,
            '—Å—É–±–±–æ—Ç–∞': 5,
            '–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ': 6,
        }
        mw = re.match(r"^([–ê-–Ø–∞-—è]+)\s*(?:,|\s)\s*\d{1,2}:\d{2}", t)
        if mw:
            wd_name = mw.group(1).lower()
            if wd_name in weekdays:
                target = weekdays[wd_name]
                diff = (today.weekday() - target) % 7
                if diff == 0:
                    diff = 7  # –≤—Å–µ–≥–¥–∞ –ø—Ä–æ—à–ª–∞—è –Ω–µ–¥–µ–ª—è, –µ—Å–ª–∏ —Å–æ–≤–ø–∞–ª —Å —Å–µ–≥–æ–¥–Ω—è—à–Ω–∏–º –¥–Ω—ë–º –Ω–µ–¥–µ–ª–∏
                dt = today - timedelta(days=diff)
                return dt.strftime("%Y-%m-%d")
        m = re.search(r"(\d{1,2})\s+(\w+)\s+(\d{4})", t)
        if m:
            day = int(m.group(1))
            month_name = m.group(2).lower()
            year = int(m.group(3))
            month = self.months_ru.get(month_name)
            if month:
                try:
                    return datetime(year, month, day).strftime("%Y-%m-%d")
                except Exception:
                    return t
        return t

    def parse_list_page(self, html: str):
        soup = BeautifulSoup(html, 'html.parser')
        articles = soup.find_all('article', class_='news')
        items = []
        for art in articles:
            try:
                a = art.find('a')
                if not a:
                    continue
                title_el = art.select_one('.title h5')
                title = title_el.get_text(strip=True) if title_el else a.get('title') or ''
                href = a.get('href') or ''
                url = href if href.startswith('http') else (self.base_url + href)
                img_div = art.select_one('.img')
                image_url = ''
                if img_div and img_div.has_attr('style'):
                    style = img_div['style']
                    m = re.search(r"url\(\"?([^\)\"]+)\"?\)", style)
                    if m:
                        src = m.group(1)
                        image_url = src if src.startswith('http') else (self.base_url + src)
                date_span = art.select_one('.date span[title="–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"]') or art.select_one('.date span')
                date_text = date_span.get_text(strip=True) if date_span else ''
                parsed_date = self.parse_date_text(date_text)
                has_kw, found_kw = self.check_keywords_in_title(title)
                items.append({
                    'title': title,
                    'url': url,
                    'date': parsed_date,
                    'date_original': date_text,
                    'author': '',
                    'image_url': image_url,
                    'has_search_keywords': has_kw,
                    'found_keywords': found_kw,
                    'has_kw_title': has_kw,
                    'found_keywords_title': list(found_kw) if found_kw else [],
                    'parsed_at': datetime.now().isoformat(),
                })
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ —Å—Ç–∞—Ç—å–∏: {e}")
                continue
        return items

    def create_news_table(self, doc, news_with_keywords, company_config):
        if not news_with_keywords:
            return None
        site_domain = company_config.get('domain', 'astrobl.ru')
        logo_path = company_config.get('logo_path', '../static/logos/logo_juka.png')
        table = doc.add_table(rows=1, cols=4)
        table.style = 'Table Grid'
        headers = [
            '–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ä–∞–¥–∏–æ—Å—Ç–∞–Ω—Ü–∏–∏',
            '–î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è –≤—ã—Ö–æ–¥–∞ –≤ —ç—Ñ–∏—Ä',
            '–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã',
            '–ù–∞–∑–≤–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞',
        ]
        for i, txt in enumerate(headers):
            cell = table.rows[0].cells[i]
            cell.text = txt
            cell.vertical_alignment = WD_ALIGN_VERTICAL.CENTER
            for p in cell.paragraphs:
                for r in p.runs:
                    r.font.bold = True
                if i < 3:
                    p.alignment = 1
        def _format_display_date(date_str: str, original_str: str) -> str:
            if date_str:
                # ISO yyyy-mm-dd
                m = re.match(r"^(\d{4}-\d{2}-\d{2})$", date_str)
                if m:
                    try:
                        return datetime.strptime(m.group(1), '%Y-%m-%d').strftime('%d.%m.%y')
                    except Exception:
                        pass
                # ISO with time yyyy-mm-dd HH:MM:SS
                m2 = re.match(r"^(\d{4}-\d{2}-\d{2})\s+\d{2}:\d{2}:\d{2}$", date_str)
                if m2:
                    try:
                        return datetime.strptime(m2.group(1), '%Y-%m-%d').strftime('%d.%m.%y')
                    except Exception:
                        pass
            # –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª
            if original_str:
                try:
                    reparsed = self.parse_date_text(original_str)
                    mm = re.match(r"^(\d{4}-\d{2}-\d{2})$", reparsed or '')
                    if mm:
                        return datetime.strptime(mm.group(1), '%Y-%m-%d').strftime('%d.%m.%y')
                except Exception:
                    pass
            return ''
        for news in news_with_keywords:
            formatted_date = _format_display_date(news.get('date') or '', news.get('date_original') or '')
            r1 = table.add_row().cells
            for c in r1:
                c.vertical_alignment = WD_ALIGN_VERTICAL.CENTER
            try:
                if os.path.exists(logo_path):
                    p = r1[0].paragraphs[0]
                    p.alignment = 1
                    run = p.runs[0] if p.runs else p.add_run()
                    run.add_picture(logo_path, width=Inches(0.5))
                else:
                    r1[0].text = '[–õ–û–ì–û]'
                    r1[0].paragraphs[0].alignment = 1
            except Exception:
                r1[0].text = '[–õ–û–ì–û]'
                r1[0].paragraphs[0].alignment = 1
            r1[1].text = formatted_date
            r1[1].paragraphs[0].alignment = 1
            r1[2].text = '¬´–ù–æ–≤–æ—Å—Ç–∏¬ª'
            r1[2].paragraphs[0].alignment = 1
            r1[3].text = news['title']
            
        return table

    def replace_variables_in_paragraph(self, paragraph, company_config, news_with_keywords, doc):
        period_text = self.format_period_ru(self.start_date, self.end_date)
        n_value = str((len(news_with_keywords) if news_with_keywords else 0) * 10)
        replacements = {
            '$pre_company_full': company_config.get('pre_company_full', ''),
            '$company_short': company_config.get('company_short', ''),
            '$post_company_full': company_config.get('post_company_full', ''),
            '$date': period_text,
            '$legal_address': company_config.get('legal_address', ''),
            '$inn_kpp': company_config.get('inn_kpp', ''),
            '$email': company_config.get('email', ''),
            '$ceo_name': company_config.get('ceo_name', ''),
            '$ogrn': company_config.get('ogrn', ''),
            '$n': n_value,
        }
        text = paragraph.text
        if '$report' in text:
            parts = text.split('$report')
            before = parts[0]
            after = parts[1] if len(parts) > 1 else ''
            for k, v in replacements.items():
                before = before.replace(k, v)
                after = after.replace(k, v)
            for r in paragraph.runs:
                r.text = ''
            if paragraph.runs:
                paragraph.runs[0].text = before
            else:
                paragraph.add_run(before)
            pe = paragraph._element
            parent = pe.getparent()
            idx = list(parent).index(pe)
            table = self.create_news_table(doc, news_with_keywords, company_config or {})
            if table is not None:
                parent.insert(idx + 1, table._element)
            if after.strip():
                new_p = doc.add_paragraph(after)
                parent.insert(idx + 2, new_p._element)
            return True
        new_text = text
        changed = False
        for k, v in replacements.items():
            if k in new_text:
                new_text = new_text.replace(k, v)
                changed = True
        if changed:
            for r in paragraph.runs:
                r.text = ''
            if paragraph.runs:
                paragraph.runs[0].text = new_text
            else:
                paragraph.add_run(new_text)
        return changed

    def replace_variables_in_cell(self, cell, company_config, news_with_keywords, doc):
        for p in cell.paragraphs:
            self.replace_variables_in_paragraph(p, company_config, news_with_keywords, doc)

    def process_template_document(self, news_with_keywords, company_name='–Æ–º–æ—Ä'):
        try:
            company_config = self.config.get('reports', {}).get('docs', {}).get(company_name, {})
            template_paths = [
                "static/template2.docx",
                "../static/template2.docx",
            ]
            template_path = None
            for path in template_paths:
                if os.path.exists(path):
                    template_path = path
                    break
            if not template_path:
                print("‚ùå template2.docx –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return None
            os.makedirs(self.reports_path, exist_ok=True)
            out_name = f"parsed_{company_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx"
            out_path = os.path.join(self.reports_path, out_name)
            shutil.copy2(template_path, out_path)
            doc = Document(out_path)
            paragraphs = list(doc.paragraphs)
            for p in paragraphs:
                if p.text.strip():
                    self.replace_variables_in_paragraph(p, company_config, news_with_keywords, doc)
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if cell.text.strip():
                            self.replace_variables_in_cell(cell, company_config, news_with_keywords, doc)
            doc.save(out_path)
            print(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {out_path}")
            return out_path
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return None

    def run(self):
        print("–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ astrobl.ru...")
        print(f"–ü–µ—Ä–∏–æ–¥: {self.start_date} ‚Äî {self.end_date}")
        period_param = self.build_period_param()
        try:
            start_dt = datetime.strptime(self.start_date, "%Y-%m-%d")
        except Exception:
            start_dt = None
        try:
            end_dt = datetime.strptime(self.end_date, "%Y-%m-%d")
        except Exception:
            end_dt = None
        page = 0
        all_items = []
        seen = set()
        while True:
            url = f"{self.list_url}?page={page}&period={period_param}"
            print(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: {url}")
            html = self.get(url)
            if not html:
                print("‚ùå –ù–µ—Ç HTML, –ø—Ä–µ—Ä—ã–≤–∞–µ–º")
                break
            items = self.parse_list_page(html)
            if not items:
                print("üèÅ –ö–æ–Ω—Ç–µ–Ω—Ç –∑–∞–∫–æ–Ω—á–∏–ª—Å—è, –≤—ã—Ö–æ–¥–∏–º")
                break
            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ø–µ—Ä–∏–æ–¥—É –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Ä–∞–Ω–Ω–∏–π –≤—ã—Ö–æ–¥
            page_has_newer = False
            page_has_in_range = False
            page_has_older = False
            filtered_items = []
            for it in items:
                item_dt = None
                d = it.get('date') or ''
                if re.match(r"\d{4}-\d{2}-\d{2}", d):
                    try:
                        item_dt = datetime.strptime(d, '%Y-%m-%d')
                    except Exception:
                        item_dt = None
                if item_dt is not None and end_dt is not None and item_dt > end_dt:
                    page_has_newer = True
                    continue
                if item_dt is not None and start_dt is not None and item_dt < start_dt:
                    page_has_older = True
                    continue
                page_has_in_range = True
                filtered_items.append(it)
            if not page_has_in_range and page_has_older:
                print("üèÅ –î–æ—Å—Ç–∏–≥–ª–∏ –Ω–∞—á–∞–ª–∞ –ø–µ—Ä–∏–æ–¥–∞, –≤—ã—Ö–æ–¥–∏–º")
                break
            print(f"–ó–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ (–≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ): {len(filtered_items)}")
            for it in filtered_items:
                print(f"  - {it.get('date','')} | {it.get('title','')}")
            new_cnt = 0
            for it in filtered_items:
                key = (it['title'], it['url'])
                if key not in seen:
                    seen.add(key)
                    all_items.append(it)
                    new_cnt += 1
            print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {new_cnt}")
            page += 1
            time.sleep(0.8)
            if page > 100:
                print("‚ö†Ô∏è –õ–∏–º–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü 100")
                break
        print(f"–ò—Ç–æ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(all_items)}")
        # –û–±–æ–≥–∞—â–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∑–∞ —Å—á–µ—Ç —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏, —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
        for it in all_items:
            try:
                if not it.get('has_search_keywords'):
                    print(f"–ü—Ä–æ–≤–µ—Ä—è—é —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏: {it.get('url','')}")
                    body_text = self.fetch_article_text(it['url'])
                    
                    has_kw_body, found_kw_body = self.check_keywords_in_text(body_text)
                    if has_kw_body:
                        print(f"  ‚úî –ù–∞–π–¥–µ–Ω–æ –≤ —Ç–µ–∫—Å—Ç–µ: {', '.join(found_kw_body)}")
                        it['has_search_keywords'] = True
                        existed = set(it.get('found_keywords') or [])
                        for kw in found_kw_body:
                            if kw not in existed:
                                existed.add(kw)
                        it['found_keywords'] = list(existed)
                    else:
                        print("  ‚úñ –ö–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            except Exception as e:
                print(f"  ‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–æ—Ä–µ —Ç–µ–∫—Å—Ç–∞: {e}")
        news_with_kw = [x for x in all_items if x.get('has_search_keywords')]
        print(f"–í –æ—Ç—á–µ—Ç –ø–æ–ø–∞–¥—É—Ç ({len(news_with_kw)}):")
        for it in news_with_kw:
            print(f"  ‚Üí {it.get('date','')} | {it.get('title','')} | keys: {', '.join(it.get('found_keywords') or [])}")
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –æ—Ç —Å–∞–º—ã—Ö —Å—Ç–∞—Ä—ã—Ö –∫ —Å–∞–º—ã–º –Ω–æ–≤—ã–º –ø–æ ISO-–¥–∞—Ç–µ (YYYY-MM-DD)
        def _date_key(item):
            d = item.get('date') or ''
            if re.match(r"\d{4}-\d{2}-\d{2}", d):
                try:
                    return datetime.strptime(d, '%Y-%m-%d')
                except Exception:
                    return datetime.min
            return datetime.min
        news_with_kw.sort(key=_date_key)
        print(f"–°–æ–≤–ø–∞–¥–µ–Ω–∏–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {len(news_with_kw)}")
        self.process_template_document(news_with_kw, company_name='–Æ–º–æ—Ä')


def main():
    JokeParser().run()


if __name__ == "__main__":
    main()